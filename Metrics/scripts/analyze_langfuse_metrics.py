#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫ Langfuse –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python analyze_langfuse_metrics.py [--export-json] [--export-txt] [--limit N]

–û–ø—Ü–∏–∏:
    --export-json    –≠–∫—Å–ø–æ—Ä—Ç –º–µ—Ç—Ä–∏–∫ –≤ langfuse_metrics.json
    --export-txt     –≠–∫—Å–ø–æ—Ä—Ç –æ—Ç—á—ë—Ç–∞ –≤ langfuse_report.txt
    --limit N        –ü–æ–ª—É—á–∏—Ç—å N —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–æ–∫ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 100, –º–∞–∫—Å: 100)

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
    - LANGFUSE_PUBLIC_KEY –∏ LANGFUSE_SECRET_KEY –≤ .env.local
"""

import os
import sys
import json
from datetime import datetime
from collections import defaultdict
from typing import Dict, List
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è - –ø—Ä–æ–±—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—É—Ç–µ–π
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(script_dir))
env_paths = [
    os.path.join(project_root, '.env.local'),
    os.path.join(script_dir, '.env.local'),
    '.env.local'
]
for env_path in env_paths:
    if os.path.exists(env_path):
        load_dotenv(dotenv_path=env_path)
        break

try:
    from langfuse import Langfuse
except ImportError:
    print("‚ùå langfuse –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –í—ã–ø–æ–ª–Ω–∏—Ç–µ: pip install langfuse")
    sys.exit(1)

# –†–∞–∑–±–æ—Ä –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
EXPORT_JSON = "--export-json" in sys.argv
EXPORT_TXT = "--export-txt" in sys.argv
LIMIT = 100  # –ú–∞–∫—Å–∏–º—É–º Langfuse API
for i, arg in enumerate(sys.argv):
    if arg == "--limit" and i + 1 < len(sys.argv):
        try:
            LIMIT = min(int(sys.argv[i + 1]), 100)  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ 100
        except ValueError:
            pass

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Langfuse
langfuse = Langfuse(
    public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
    secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
    host=os.getenv("LANGFUSE_HOST", "https://cloud.langfuse.com")
)

def analyze_metrics(days=7):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ Langfuse –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –¥–Ω–µ–π."""
    
    output_lines = []
    def log(msg=""):
        print(msg)
        output_lines.append(msg)
    
    log(f"\nüìä –ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫ Langfuse (–ø–æ—Å–ª–µ–¥–Ω–∏–µ {days} –¥–Ω–µ–π, –ª–∏–º–∏—Ç={LIMIT})...")
    log("=" * 60)
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–æ–∫
    log("–ó–∞–≥—Ä—É–∑–∫–∞ —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–æ–∫ –∏–∑ Langfuse...")
    traces = langfuse.fetch_traces(limit=LIMIT)
    
    if not traces.data:
        log("‚ùå –¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤ Langfuse –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ.")
        return output_lines, {}
    
    log(f"–ù–∞–π–¥–µ–Ω–æ {len(traces.data)} —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–æ–∫\n")
    
    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫
    latencies = defaultdict(list)
    token_usage = {"input": [], "output": [], "total": []}
    operation_counts = defaultdict(int)
    trace_details = []
    
    for trace in traces.data:
        trace_info = {
            "id": getattr(trace, 'id', None),
            "name": getattr(trace, 'name', "unknown"),
            "timestamp": str(getattr(trace, 'timestamp', None)),
            "latency": None,
            "tokens": {},
            "metadata": getattr(trace, 'metadata', {})
        }
        
        if hasattr(trace, 'name'):
            operation_counts[trace.name] += 1
        
        if hasattr(trace, 'latency') and trace.latency:
            latencies[trace.name or "unknown"].append(trace.latency)
            trace_info["latency"] = trace.latency
        
        if hasattr(trace, 'usage') and trace.usage:
            if hasattr(trace.usage, 'input'):
                inp = trace.usage.input or 0
                token_usage["input"].append(inp)
                trace_info["tokens"]["input"] = inp
            if hasattr(trace.usage, 'output'):
                out = trace.usage.output or 0
                token_usage["output"].append(out)
                trace_info["tokens"]["output"] = out
            if hasattr(trace.usage, 'total'):
                tot = trace.usage.total or 0
                token_usage["total"].append(tot)
                trace_info["tokens"]["total"] = tot
        
        trace_details.append(trace_info)
    
    # === –ê–ù–ê–õ–ò–ó LATENCY ===
    log("\n‚è±Ô∏è  –ê–ù–ê–õ–ò–ó –ó–ê–î–ï–†–ñ–ö–ò (LATENCY)")
    log("-" * 60)
    
    if latencies:
        for op, times in sorted(latencies.items()):
            if times:
                avg = sum(times) / len(times)
                p95 = sorted(times)[int(len(times) * 0.95)] if len(times) > 1 else times[0]
                log(f"  {op:20} | —Å—Ä–µ–¥–Ω–µ–µ: {avg:.2f}–º—Å | p95: {p95:.2f}–º—Å | –∫–æ–ª-–≤–æ: {len(times)}")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        log("\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ latency:")
        for op, times in latencies.items():
            avg = sum(times) / len(times)
            if op == "gemini_generate" and avg > 5000:
                log(f"  ‚ö†Ô∏è  {op} –º–µ–¥–ª–µ–Ω–Ω—ã–π ({avg:.0f}–º—Å). –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ:")
                log(f"     - –£–º–µ–Ω—å—à–∏—Ç–µ LG_PROMPT_TEXT_CHARS (—Ç–µ–∫—É—â–µ–µ —Ä–µ–∫.: 8000-10000)")
                log(f"     - –£–º–µ–Ω—å—à–∏—Ç–µ LG_NOTES_SHOW_MAX (—Ç–µ–∫—É—â–µ–µ —Ä–µ–∫.: 6-8)")
                log(f"     - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ GEMINI_MODEL=gemini-2.5-flash-lite")
            elif op == "rag_retrieve" and avg > 1000:
                log(f"  ‚ö†Ô∏è  {op} –º–µ–¥–ª–µ–Ω–Ω—ã–π ({avg:.0f}–º—Å). –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ:")
                log(f"     - –£–º–µ–Ω—å—à–∏—Ç–µ RAG_TOP_K (—Ç–µ–∫—É—â–µ–µ: 8, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ 5-6)")
                log(f"     - –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–∞–∑–º–µ—Ä RAG_INDEX_DIR (–º–Ω–æ–≥–æ —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö?)")
            elif op == "exa_search" and avg > 4000:
                log(f"  ‚ö†Ô∏è  {op} –º–µ–¥–ª–µ–Ω–Ω—ã–π ({avg:.0f}–º—Å). –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ:")
                log(f"     - –£–º–µ–Ω—å—à–∏—Ç–µ EXA_NUM_RESULTS (—Ä–µ–∫.: 4-6)")
                log(f"     - –£–º–µ–Ω—å—à–∏—Ç–µ EXA_GET_CONTENTS_N (—Ä–µ–∫.: 2)")
                log(f"     - –£–º–µ–Ω—å—à–∏—Ç–µ LG_EXA_TIME_BUDGET_S (—Ä–µ–∫.: 3.0-4.0)")
                log(f"     - –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ EXA_RESEARCH_ENABLED=0 (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)")
    else:
        log("  –î–∞–Ω–Ω—ã–µ –æ latency –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã (—Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏ –º–æ–≥—É—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç—å latency)")
    
    # === –ê–ù–ê–õ–ò–ó –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø –¢–û–ö–ï–ù–û–í ===
    log("\nüî¢ –ê–ù–ê–õ–ò–ó –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø –¢–û–ö–ï–ù–û–í")
    log("-" * 60)
    
    if token_usage["total"]:
        avg_input = sum(token_usage["input"]) / len(token_usage["input"]) if token_usage["input"] else 0
        avg_output = sum(token_usage["output"]) / len(token_usage["output"]) if token_usage["output"] else 0
        avg_total = sum(token_usage["total"]) / len(token_usage["total"]) if token_usage["total"] else 0
        
        log(f"  –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –∑–∞–ø—Ä–æ—Å:")
        log(f"    –í—Ö–æ–¥–Ω—ã–µ:  {avg_input:.0f} —Ç–æ–∫–µ–Ω–æ–≤")
        log(f"    –í—ã—Ö–æ–¥–Ω—ã–µ: {avg_output:.0f} —Ç–æ–∫–µ–Ω–æ–≤")
        log(f"    –í—Å–µ–≥–æ:    {avg_total:.0f} —Ç–æ–∫–µ–Ω–æ–≤")
        
        # –û—Ü–µ–Ω–∫–∞ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ (–¥–ª—è gemini-2.5-flash-lite)
        cost_per_1k_input = 0.00001  # $0.01 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
        cost_per_1k_output = 0.00003  # $0.03 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
        est_cost = (avg_input * cost_per_1k_input / 1000) + (avg_output * cost_per_1k_output / 1000)
        log(f"\n  –ü—Ä–∏–º–µ—Ä–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –∑–∞ –∑–∞–ø—Ä–æ—Å: ${est_cost:.6f}")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        log("\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ç–æ–∫–µ–Ω–∞–º:")
        if avg_input > 8000:
            log(f"  ‚ö†Ô∏è  –í—ã—Å–æ–∫–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ ({avg_input:.0f}). –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ:")
            log(f"     - –£–º–µ–Ω—å—à–∏—Ç–µ LG_PROMPT_TEXT_CHARS (—Ç–µ–∫—É—â–µ–µ —Ä–µ–∫.: 8000-10000)")
            log(f"     - –£–º–µ–Ω—å—à–∏—Ç–µ LG_NOTES_SHOW_MAX (—Ç–µ–∫—É—â–µ–µ —Ä–µ–∫.: 6-8)")
            log(f"     - –£–≤–µ–ª–∏—á—å—Ç–µ RAG_TOP_K –¥–ª—è –∑–∞–º–µ–Ω—ã —Å—ã—Ä–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (—Ä–µ–∫.: 10-12)")
            log(f"     - –£–º–µ–Ω—å—à–∏—Ç–µ LG_SEARCH_SNIPPET_CHARS (—Ç–µ–∫—É—â–µ–µ —Ä–µ–∫.: 400-500)")
        if avg_output > 1000:
            log(f"  ‚ö†Ô∏è  –í—ã—Å–æ–∫–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ ({avg_output:.0f}). –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ:")
            log(f"     - –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ LG_ANSWER_MAX_SENTENCES=7 (–æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å –¥–ª–∏–Ω—É)")
            log(f"     - –°–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–π—Ç–µ —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏")
        if avg_input > 0 and avg_input < 2000:
            log(f"  ‚ÑπÔ∏è  –ù–∏–∑–∫–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ ({avg_input:.0f}).")
            log(f"     –ú–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤:")
            log(f"     - –£–≤–µ–ª–∏—á—å—Ç–µ LG_PROMPT_TEXT_CHARS (—Ä–µ–∫.: 12000-15000)")
            log(f"     - –£–≤–µ–ª–∏—á—å—Ç–µ LG_NOTES_SHOW_MAX (—Ä–µ–∫.: 10-12)")
            log(f"     - –£–≤–µ–ª–∏—á—å—Ç–µ LG_SEARCH_RESULTS_MAX (—Ä–µ–∫.: 4-5)")
    else:
        log("  –î–∞–Ω–Ω—ã–µ –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
    
    # === –ß–ê–°–¢–û–¢–ê –û–ü–ï–†–ê–¶–ò–ô ===
    log("\nüìà –ß–ê–°–¢–û–¢–ê –û–ü–ï–†–ê–¶–ò–ô")
    log("-" * 60)
    
    if operation_counts:
        total_ops = sum(operation_counts.values())
        for op, count in sorted(operation_counts.items(), key=lambda x: x[1], reverse=True):
            pct = (count / total_ops) * 100 if total_ops > 0 else 0
            log(f"  {op:20} | {count:4} —Ä–∞–∑ ({pct:.1f}%)")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        log("\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —á–∞—Å—Ç–æ—Ç–µ:")
        rag_ops = operation_counts.get("rag_retrieve", 0) + operation_counts.get("rag_upsert", 0)
        search_ops = operation_counts.get("exa_search", 0)
        gemini_ops = operation_counts.get("gemini_generate", 0)
        
        if search_ops > rag_ops * 0.5:
            log(f"  ‚ö†Ô∏è  –í—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞ ({search_ops} vs {rag_ops} RAG). –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ:")
            log(f"     - –£–≤–µ–ª–∏—á—å—Ç–µ RAG_TOP_K (—Ç–µ–∫—É—â–µ–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–µ: 12-15)")
            log(f"     - –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ RAG_ENABLED=1 (–≤–∫–ª—é—á–µ–Ω)")
            log(f"     - –£–≤–µ–ª–∏—á—å—Ç–µ LG_SEARCH_MIN_CONTEXT_CHARS (—Ä–µ–∂–µ –∑–∞–ø—É—Å–∫–∞—Ç—å –ø–æ–∏—Å–∫)")
            log(f"     - –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ LG_SEARCH_HEURISTICS=0 (–æ—Ç–∫–ª—é—á–∏—Ç—å —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –ø–æ–∏—Å–∫–∞)")
        
        if rag_ops < gemini_ops * 0.2:
            log(f"  ‚ö†Ô∏è  –ù–∏–∑–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ RAG ({rag_ops} vs {gemini_ops} Gemini). –ü—Ä–æ–≤–µ—Ä—å—Ç–µ:")
            log(f"     - RAG_ENABLED=1 (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤–∫–ª—é—á–µ–Ω)")
            log(f"     - RAG_TOP_K >= 8 (–º–∏–Ω–∏–º—É–º –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ä–∞–±–æ—Ç—ã)")
            log(f"     - RAG_INDEX_DIR –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
        
        if search_ops == 0 and gemini_ops > 10:
            log(f"  ‚ÑπÔ∏è  –ü–æ–∏—Å–∫ Exa –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è. –≠—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –µ—Å–ª–∏:")
            log(f"     - LG_SEARCH_HEURISTICS=0 (—ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –æ—Ç–∫–ª—é—á–µ–Ω—ã)")
            log(f"     - LG_SEARCH_MIN_CONTEXT_CHARS > –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü")
            log(f"     - –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –Ω–µ –∑–∞–¥–∞—é—Ç –≤–æ–ø—Ä–æ—Å—ã —Ç—Ä–µ–±—É—é—â–∏–µ –≤–Ω–µ—à–Ω–µ–≥–æ –ø–æ–∏—Å–∫–∞")
    else:
        log("  –î–∞–Ω–Ω—ã–µ –æ–± –æ–ø–µ—Ä–∞—Ü–∏—è—Ö –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
    
    # === –ò–¢–û–ì–ò ===
    log("\n" + "=" * 60)
    log("üìã –ò–¢–û–ì–ò –ò –î–ï–ô–°–¢–í–ò–Ø")
    log("=" * 60)
    log("\n–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:")
    log("  1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ params.override.json –∏ —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–π—Ç–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –≤—ã—à–µ")
    log("  2. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ endpoint /config –¥–ª—è –≥–æ—Ä—è—á–µ–π –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –±–µ–∑ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞")
    log("  3. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —ç—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç —Å–Ω–æ–≤–∞ –ø–æ—Å–ª–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫")
    log("\n–°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
    log("  - –ù–∞—Å—Ç—Ä–æ–π—Ç–µ Langfuse Datasets –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
    log("  - –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –æ—Ü–µ–Ω–∫–∏ –≤ Langfuse –¥–ª—è –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞")
    log("  - –°–æ–∑–¥–∞–π—Ç–µ –¥–∞—à–±–æ—Ä–¥—ã –≤ Langfuse –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏")
    log("")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
    export_data = {
        "timestamp": datetime.now().isoformat(),
        "total_traces": len(traces.data),
        "latency": {op: {
            "avg": sum(times) / len(times) if times else 0,
            "p95": sorted(times)[int(len(times) * 0.95)] if len(times) > 1 else (times[0] if times else 0),
            "count": len(times)
        } for op, times in latencies.items()},
        "token_usage": {
            "avg_input": sum(token_usage["input"]) / len(token_usage["input"]) if token_usage["input"] else 0,
            "avg_output": sum(token_usage["output"]) / len(token_usage["output"]) if token_usage["output"] else 0,
            "avg_total": sum(token_usage["total"]) / len(token_usage["total"]) if token_usage["total"] else 0,
        },
        "operation_counts": dict(operation_counts),
        "traces": trace_details
    }
    
    return output_lines, export_data

if __name__ == "__main__":
    try:
        output_lines, export_data = analyze_metrics(days=7)
        
        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø—É—Ç–∏ –≤—ã–≤–æ–¥–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–∞—Ç–∞–ª–æ–≥–∞ Metrics
        script_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        data_dir = os.path.join(script_dir, "data")
        reports_dir = os.path.join(script_dir, "reports")
        os.makedirs(data_dir, exist_ok=True)
        os.makedirs(reports_dir, exist_ok=True)
        
        # –≠–∫—Å–ø–æ—Ä—Ç –≤ TXT
        if EXPORT_TXT:
            txt_file = os.path.join(reports_dir, "langfuse_report.txt")
            with open(txt_file, "w", encoding="utf-8") as f:
                f.write("\n".join(output_lines))
            print(f"‚úÖ –û—Ç—á—ë—Ç —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –≤: {txt_file}")
        
        # –≠–∫—Å–ø–æ—Ä—Ç –≤ JSON
        if EXPORT_JSON:
            json_file = os.path.join(data_dir, "langfuse_metrics.json")
            with open(json_file, "w", encoding="utf-8") as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False)
            print(f"‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤: {json_file}")
        
        if EXPORT_TXT or EXPORT_JSON:
            print(f"\nüí° –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç—Ç–∏ —Ñ–∞–π–ª—ã –¥–ª—è:")
            print(f"   - –°—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
            print(f"   - –°–æ–∑–¥–∞–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å matplotlib/plotly)")
            print(f"   - –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Ç—Ä–µ–Ω–¥–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤–æ –≤—Ä–µ–º–µ–Ω–∏")
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        print("\n–£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ:")
        print("  1. LANGFUSE_PUBLIC_KEY –∏ LANGFUSE_SECRET_KEY —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –≤ .env.local")
        print("  2. –£ –≤–∞—Å –µ—Å—Ç—å —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏ –≤ Langfuse (–≤—ã–ø–æ–ª–Ω–∏—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–ø—Ä–æ—Å–æ–≤)")
        sys.exit(1)

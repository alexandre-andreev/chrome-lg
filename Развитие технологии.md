Предлагаю короткий план следующих улучшений в LangGraph, с фокусом на скорость и устойчивость.

- Гибридный роутер (эвристики + LLM)
  - Быстрые эвристики для явных кейсов (запрет/разрешение EXA, явный “найди/ищи”, фактологические паттерны).
  - LLM-классификатор для сложных случаев (general | web_search | out_of_context).
  - Fallback-эвристика при неверном JSON.

- Локальный RAG вместо “сырого” текста
  - Чанкинг страницы → эмбеддинги → локальный векторный индекс (напр., FAISS/SQLite).
  - Узел retrieve_top_k: подстановка 3–6 самых близких чанков в промпт вместо длинного TEXT.
  - Перехранение по host (коллекции на сайт), фоновая актуализация при смене страницы.

- Реранк/дедуп результатов поиска
  - Узел rerank_results_llm: объединение по домену, удаление дубликатов, оставить 3–5 лучших сниппетов.

- Тайм-бюджеты и защита
  - Жёсткие timeouts/бюджеты (EXA/LLM), ранние выходы.
  - “Circuit breaker” для EXA при серии ошибок.

- Наблюдаемость
  - /metrics: латентность узлов, решение роутера, длина контекста, токены.
  - Логирование причин ветвления (решение эвристик/LLM).

Если ок, начну с двух самых полезных шагов:
1) Гибридный роутер в начале (эвристики → LLM → fallback). -НЕ ДЕЛАЕМ ПОКА
2) Локальный RAG-узел (индекс по сайту и retrieve_top_k в промпт).- НАДО ПОПРОБОВАТЬ

### 1) Реранк/дедуп результатов: как выбирать лучшие сниппеты

- Простые сигналы (быстро и дешево)
  - совпадение домена с текущей страницей (если вопрос про объект с этой страницы);
  - плотность ключевых терминов запроса (BM25/TF-IDF на коротком тексте сниппета);
  - наличие чисел/единиц измерения, цен, дат (для фактологических вопросов);
  - длина осмысленного текста (не слишком короткий/не рекламный);
  - свежесть (если доступна дата).
- MMR/диверсификация
  - отобрать релевантные, затем “разредить” схожие сниппеты (Maximal Marginal Relevance), чтобы убрать повторы.
- LLM‑реранк (надежно и универсально)
  - узел `rerank_results_llm`: дать LLM список кандидатов с короткими правилами “оцени релевантность к вопросу и контексту страницы; верни 3–5 лучших с краткой причиной”.
  - вернуть JSON: [{url,title,snippet,score,reason}], при невалидном JSON — fallback на эвристику.
- Дедупликация
  - по (hostname, нормализованный путь) + каноникал‑URL если есть;
  - по тексту сниппета: нормализовать, убрать стоп‑слова и сравнивать косинусную близость/шинглы (выбрасывать слишком похожие).

Реально применимый порядок без тяжёлых моделей:
1) хард‑фильтр по домену/ошибкам → 2) быстрый BM25/TF‑IDF + числа → 3) MMR → 4) LLM‑реранк топ‑8 до топ‑3/5 → 5) финальная дедупликация.

### 2) Локальный RAG: нужна ли внешняя векторная БД?

- Не обязательно. Можно стартовать без внешних сервисов:
  - in‑process FAISS (быстро, просто, хранить индекс на диске per‑host);
  - SQLite с VSS/pgvector-аналога (sqlite‑vss) — лёгкая локальная зависимость;
  - даже “ручной” numpy‑индекс (cosine) на десятках–сотнях чанков страницы/сайта.
- Когда нужны внешние:
  - Supabase (Postgres + pgvector) — удобно, если хотите общий облачный индекс, нескольким клиентам, с бэкапами и правами;
  - любой управляемый pgvector/Weaviate/Pinecone — при больших объёмах и онлайновом шаринге.
- Для нашего кейса (локально на ноутбуке, быстрый ответ по текущему сайту):
  - достаточно локального индекса per‑host (папка `.rag_index/<hostname>/...`) на FAISS или SQLite‑VSS;
  - хранить эмбеддинги только текущей/недавних страниц; TTL или LRU на уровне файлов.

Итоговые рекомендации:
- Реранк: начните с гибридного (эвристика + LLM‑реранк для топ‑8 → топ‑3/5) и MMR‑дедуп.
- RAG: сначала локальный FAISS/SQLite‑VSS per‑host (без внешней БД). Если появится потребность в шаринге и масштабировании — перенесём в Supabase (pgvector).


Коротко как сделаем локальный RAG per-host с минимальными зависимостями:

- Библиотека
  - Попробуем `faiss-cpu` через pip. Если на Windows/Python 3.13 колёс нет — автоматически переключимся на лёгкий fallback `annoy` (никаких действий от вас не потребуется).

- Эмбеддинги
  - Без локальных моделей: используем embeddings из Google AI (`text-embedding-004`) через уже настроенный `google.generativeai`, чтобы не тянуть torch/sentence-transformers.

- Индекс per-host на диске
  - Папка: `backend_lg/.rag_index/<host>/`
  - Файлы: `index.faiss` (или `index.ann` для fallback), `meta.jsonl` (id → текст/источник).
  - Лимиты: cap по количеству чанков (например, 2–5 тыс.), LRU/TTL на уровне файлов.

- Встраивание в граф
  - Узел `rag_upsert`: на входе длинный `page.text` → чанкование (напр. 800–1000 симв, overlap 100–150) → эмбедды → upsert в индекс для текущего host.
  - Узел `rag_retrieve`: эмбеддинг запроса → top-k (3–6) чанков → вместо длинного `TEXT` в `compose_prompt` подставляем секцию RAG (`RAG CONTEXT:\n- ...`) + оставляем NOTES при наличии.
  - Флаги ENV: `RAG_ENABLED=1`, `RAG_TOP_K=5`, `RAG_CHUNK_SIZE=900`, `RAG_OVERLAP=120`, `RAG_MAX_DOCS_PER_HOST=5000`.

- Поведение и скорость
  - На первой странице/хосте: единовременное построение индекса (несколько сотен чанков быстро; сетевые вызовы на эмбедды).
  - На последующих запросах по тому же хосту: быстрый top‑k без EXA и без длинного `TEXT` — ответ быстрее и стабильнее.

- Установка
  - Сначала попробуем: `pip install faiss-cpu`
  - Если не доступно (Windows/Python 3.13): автоматически используем `annoy` (`pip install annoy`), код это обработает сам.

Готов внедрить это так, чтобы у вас ничего не сломалось: включение только флагом `RAG_ENABLED=1`; без флага система работает как сейчас.